{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9edc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export PYTHONPATH=/v/xgrp0j/pnet_prostate_paper:$PYTHONPATH\n",
    "!export PYTHONPATH=~/pnet_prostate_paper:$PYTHONPATH\n",
    "from os.path import dirname, realpath\n",
    "import sys\n",
    "import kaleido\n",
    "print(sys.version)\n",
    "from analysis.vis_utils import get_reactome_pathway_names\n",
    "#from config_path import PATHWAY_PATH\n",
    "from analysis.figure_3.setup import saving_dir\n",
    "\n",
    "current_dir = dirname(realpath(\"__file__\"))\n",
    "module_path = current_dir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from os.path import dirname, realpath\n",
    "\n",
    "from analysis.vis_utils import get_reactome_pathway_names\n",
    "#from config_path import PATHWAY_PATH\n",
    "from analysis.figure_3.setup import saving_dir\n",
    "\n",
    "current_dir = dirname(realpath(\"__file__\"))\n",
    "module_path = current_dir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from utils.xai_age_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce188e",
   "metadata": {},
   "source": [
    "# Generate the Sankey diagram from the extracted folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ff74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get reactome pathway ids and names\n",
    "#REACTOM_PATHWAY_PATH = \"/_database/pathways/Reactome/\"\n",
    "reactome_pathway_df = get_reactome_pathway_names()\n",
    "id_to_name_dict = dict(zip(reactome_pathway_df.id, reactome_pathway_df.name))\n",
    "name_to_id_dict = dict(zip(reactome_pathway_df.name, reactome_pathway_df.id))\n",
    "\n",
    "# nlargest= [10, 8, 8, 8, 7, 6]\n",
    "nlargest = [5, 5, 5, 5, 5, 5, 5]\n",
    "# nlargest= 10\n",
    "\n",
    "node_importance = pd.read_csv(join(module_path, './extracted/node_importance_graph_adjusted.csv'), index_col=0)\n",
    "#####\n",
    "# node_importance.coef_combined = node_importance.coef\n",
    "\n",
    "node_id = []\n",
    "for x in node_importance.index:\n",
    "    if x in name_to_id_dict.keys():\n",
    "        node_id.append(name_to_id_dict[x])\n",
    "    else:\n",
    "        node_id.append(x)\n",
    "node_importance['node_id'] = node_id\n",
    "\n",
    "col_name = 'coef'\n",
    "\n",
    "first_layer_nodes = node_importance[node_importance.layer == 1].copy()\n",
    "other_layer_nodes = node_importance[node_importance.layer != 1].copy()\n",
    "high_nodes_first_layer = get_high_nodes(first_layer_nodes, nlargest=nlargest, column='coef_combined')\n",
    "# high_nodes_first_layer = get_high_nodes(first_layer_nodes, nlargest=nlargest, column='coef')\n",
    "high_nodes_pathways = get_high_nodes(other_layer_nodes, nlargest=nlargest, column='coef')\n",
    "# high_nodes_pathways= get_high_nodes(other_layer_nodes, nlargest=nlargest, column='coef_combined')\n",
    "high_nodes = high_nodes_first_layer + high_nodes_pathways\n",
    "print('high_nodes', high_nodes)\n",
    "high_nodes_df = filter_nodes(node_importance, high_nodes)\n",
    "\n",
    "high_nodes_ids = list(high_nodes_df.node_id.values)\n",
    "\n",
    "links_df = get_links()\n",
    "# links_df = get_links_with_first_layer()\n",
    "\n",
    "'''\n",
    "MDM4\n",
    "'''\n",
    "mdm4_nodes = get_MDM4_nodes(links_df)\n",
    "mdm4_nodes_names = []\n",
    "for n in mdm4_nodes:\n",
    "    if n in id_to_name_dict.keys():\n",
    "        mdm4_nodes_names.append(id_to_name_dict[n])\n",
    "    else:\n",
    "        mdm4_nodes_names.append(n)\n",
    "\n",
    "print('mdm4_nodes', mdm4_nodes_names)\n",
    "\n",
    "ind = links_df.source == links_df.target\n",
    "links_df = links_df[~ind]\n",
    "\n",
    "# # keep important nodes only\n",
    "links_df = filter_connections(links_df, high_nodes_ids, add_unk=True)\n",
    "\n",
    "links_df = links_df.reset_index()\n",
    "\n",
    "# print links_df.head()\n",
    "links_df['value_abs'] = links_df.value.abs()\n",
    "\n",
    "links_df['child_sum_target'] = links_df.groupby('target').value_abs.transform(np.sum)\n",
    "links_df['child_sum_source'] = links_df.groupby('source').value_abs.transform(np.sum)\n",
    "links_df['value_normalized_by_target'] = 100 * links_df.value_abs / links_df.child_sum_target\n",
    "links_df['value_normalized_by_source'] = 100 * links_df.value_abs / links_df.child_sum_source\n",
    "\n",
    "#\n",
    "node_importance['coef_combined_normalized_by_layer'] = 100. * node_importance[col_name] / \\\n",
    "                                                       node_importance.groupby('layer')[col_name].transform(np.sum)\n",
    "\n",
    "node_importance_ = node_importance[['node_id', 'coef_combined_normalized_by_layer', col_name]].copy()\n",
    "#\n",
    "#\n",
    "node_importance_['coef_combined_normalized_by_layer'] = np.log(\n",
    "    1. + node_importance_.coef_combined_normalized_by_layer)\n",
    "node_importance_normalized = node_importance_[['node_id', 'coef_combined_normalized_by_layer']]\n",
    "node_importance_normalized = node_importance_normalized.set_index('node_id')\n",
    "node_importance_normalized.columns = ['target_importance']\n",
    "#\n",
    "links_df_ = pd.merge(links_df, node_importance_normalized, left_on='target', right_index=True, how='left')\n",
    "node_importance_normalized.columns = ['source_importance']\n",
    "links_df_ = pd.merge(links_df_, node_importance_normalized, left_on='source', right_index=True, how='left')\n",
    "\n",
    "#\n",
    "def adjust_values(links_df_in):\n",
    "    df = links_df_in.copy()\n",
    "    df['A'] = df.value_normalized_by_source * df.source_importance\n",
    "    df['B'] = df.value_normalized_by_target * df.target_importance\n",
    "    df['value_final'] = df[[\"A\", \"B\"]].min(axis=1)\n",
    "    #\n",
    "    df['value_old'] = df.value\n",
    "    df.value = df.value_final\n",
    "    #\n",
    "    df['source_fan_out'] = df.groupby('source').value_final.transform(np.sum)\n",
    "    df['source_fan_out_error'] = np.abs(df.source_fan_out - 100. * df.source_importance)\n",
    "\n",
    "    df['target_fan_in'] = df.groupby('target').value_final.transform(np.sum)\n",
    "    df['target_fan_in_error'] = np.abs(df.target_fan_in - 100. * df.target_importance)\n",
    "    #\n",
    "    #\n",
    "    ind = df.source.str.contains('others')\n",
    "    df['value_final_corrected'] = df.value_final\n",
    "    df.loc[ind, 'value_final_corrected'] = df[ind].value_final + df[ind].target_fan_in_error\n",
    "    ind = df.target.str.contains('others')\n",
    "\n",
    "    df.loc[ind, 'value_final_corrected'] = df[ind].value_final_corrected + df[ind].source_fan_out_error\n",
    "\n",
    "    df.value = df.value_final_corrected\n",
    "    return df\n",
    "\n",
    "df = adjust_values(links_df_)\n",
    "# df.to_csv('links_df.csv')\n",
    "important_node_connections_df = df.replace(id_to_name_dict)\n",
    "\n",
    "# important_node_connections_df.to_csv('important_node_connections_df.csv')\n",
    "# high_nodes_df.to_csv('high_nodes_df.csv')\n",
    "\n",
    "high_nodes_df = high_nodes_df[[col_name, 'layer']]\n",
    "\n",
    "# add feature nodes\n",
    "high_nodes_df.loc['mutation'] = [1, 0]\n",
    "high_nodes_df.loc['amplification'] = [1, 0]\n",
    "high_nodes_df.loc['deletion'] = [1, 0]\n",
    "high_nodes_df.loc['methylation'] = [1, 0]\n",
    "# high_nodes_df.loc['other1'] = [1, 1]\n",
    "# high_nodes_df.loc['hidden'] = [1, 8]\n",
    "\n",
    "# add first layer\n",
    "first_layer_df = get_first_layer_df(nlargest)\n",
    "links_df = pd.concat([first_layer_df, important_node_connections_df], sort=True).reset_index()\n",
    "\n",
    "\n",
    "linkes_filtred_, nodes_df = get_fromated_network(links_df, high_nodes_df, col_name=col_name, remove_others=False)\n",
    "\n",
    "scale = 1.\n",
    "width = 600. / scale\n",
    "height = 0.5 * width / scale\n",
    "# linkes_filtred_.to_csv('linkes_filtred.csv')\n",
    "# nodes_df.to_csv('nodes_df.csv')\n",
    "data_trace, layout = get_data_trace(linkes_filtred_, nodes_df, height, width)\n",
    "fig = dict(data=[data_trace], layout=layout)\n",
    "fig = go.Figure(fig)\n",
    "filename = join(saving_dir, 'sankey_print.pdf')\n",
    "fig.write_image(filename, scale=1, width=width, height=height, format='pdf')\n",
    "\n",
    "filename = join(saving_dir, 'sankey_print.png')\n",
    "fig.write_image(filename, scale=5, width=width, height=height, format='png')\n",
    "\n",
    "from plotly.offline import plot\n",
    "scale = 0.5\n",
    "width = 600. / scale\n",
    "height = 0.5 * width\n",
    "data_trace, layout = get_data_trace(linkes_filtred_, nodes_df, height, width, fontsize=12)\n",
    "fig = dict(data=[data_trace], layout=layout)\n",
    "filename = 'sankey_youngblood.html'\n",
    "filename = join(saving_dir, filename)\n",
    "plot(fig, filename=filename)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585be7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this app with `python app.py` and\n",
    "# visit http://127.0.0.1:8050/ in your web browser.\n",
    "\n",
    "\n",
    "\n",
    "app = Dash(__name__)\n",
    "\n",
    "# assume you have a \"long-form\" data frame\n",
    "# see https://plotly.com/python/px-arguments/ for more options\n",
    "df = pd.DataFrame({\n",
    "    \"Fruit\": [\"Apples\", \"Oranges\", \"Bananas\", \"Apples\", \"Oranges\", \"Bananas\"],\n",
    "    \"Amount\": [4, 1, 2, 2, 4, 5],\n",
    "    \"City\": [\"SF\", \"SF\", \"SF\", \"Montreal\", \"Montreal\", \"Montreal\"]\n",
    "})\n",
    "\n",
    "fig = px.bar(df, x=\"Fruit\", y=\"Amount\", color=\"City\", barmode=\"group\")\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(children='Hello Dash'),\n",
    "\n",
    "    html.Div(children='''\n",
    "        Dash: A web application framework for your data.\n",
    "    '''),\n",
    "\n",
    "    dcc.Graph(\n",
    "        id='example-graph',\n",
    "        figure=fig\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #app.run_server(debug=True)\n",
    "    app.run_server(debug=True, use_reloader=False,port=8052)  <---- Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67ad92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a386f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970184b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
