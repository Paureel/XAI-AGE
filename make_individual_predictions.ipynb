{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4bf7ef",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca90c49",
   "metadata": {},
   "source": [
    "To make inferences with the model, first you need to format and place your files in the following directories:\n",
    "\n",
    "- _database/processed/P1000_adjusted_TPM.csv This file will contain your sample ID-s and CpG-gene beta values.\n",
    "- _database/processed/response_paper.csv This file contains the sample ID-s and the corresponding age.\n",
    "- _database/splits/ This directory contains the train-test-validation set for the datasets, and by default only contains the test set of the pan-tissue dataset (even if it is named as train set). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa9d72",
   "metadata": {},
   "source": [
    "# Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d13536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os.path import join, dirname, realpath\n",
    "#current_dir = dirname(realpath(__file__))\n",
    "current_dir = \"train\"\n",
    "from preprocessing import pre\n",
    "import subprocess\n",
    "sys.path.insert(0, dirname(current_dir))\n",
    "import os\n",
    "import imp\n",
    "import logging\n",
    "import random\n",
    "import timeit\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.logs import set_logging, DebugFolder\n",
    "import yaml\n",
    "from pipeline.train_validate import TrainValidatePipeline\n",
    "from pipeline.one_split import OneSplitPipeline\n",
    "from pipeline.crossvalidation_pipeline import CrossvalidationPipeline\n",
    "from pipeline.LeaveOneOut_pipeline import LeaveOneOutPipeline\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "#from data.data_access import Data\n",
    "from data.prostate_paper.data_reader import ProstateDataPaper\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso, SGDClassifier, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from analysis.figure_3.data_extraction_utils import get_node_importance, get_link_weights_df_, \\\n",
    "    get_data, get_degrees, adjust_coef_with_graph_degree, get_pathway_names\n",
    "from model.coef_weights_utils import get_deep_explain_scores\n",
    "from os import makedirs\n",
    "from os.path import dirname, realpath, exists\n",
    "import pickle\n",
    "from model.model_utils import get_coef_importance\n",
    "from model import nn\n",
    "from analysis.figure_3.data_extraction_utils import get_node_importance, get_link_weights_df_, \\\n",
    "    get_data, get_degrees, adjust_coef_with_graph_degree, get_pathway_names\n",
    "from utils.loading_utils import DataModelLoader\n",
    "from xai_age_utils import *\n",
    "#from config_path import PROSTATE_LOG_PATH, POSTATE_PARAMS_PATH\n",
    "LOG_PATH = \"_logs/XAI-AGE/RUN1/crossvalidation_average_reg_10_tanh\"\n",
    "POSTATE_PARAMS_PATH = \"train/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fb543",
   "metadata": {},
   "source": [
    "# Define the type of run (a modified crossvalidation_average_reg_10_tanh by default based on Elmarakeby et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file_list = []\n",
    "\n",
    "\n",
    "params_file_list.append('./crossvalidation_average_reg_10_tanh')\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "random_seed = 234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_random_seed(random_seed)\n",
    "\n",
    "timeStamp = '_{0:%b}-{0:%d}_{0:%H}-{0:%M}'.format(datetime.datetime.now())\n",
    "def elapsed_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "for params_file in params_file_list:\n",
    "    log_dir = join(LOG_PATH, params_file)\n",
    "    log_dir = log_dir\n",
    "    set_logging(log_dir)\n",
    "    params_file = join(POSTATE_PARAMS_PATH, params_file)\n",
    "    logging.info('random seed %d' % random_seed)\n",
    "    params_file_full = params_file + '.py'\n",
    "    print params_file_full\n",
    "    params = imp.load_source(params_file, params_file_full)\n",
    "\n",
    "    DebugFolder(log_dir)\n",
    "    if params.pipeline['type'] == 'one_split':\n",
    "        pipeline = OneSplitPipeline(task=params.task, data_params=params.data, model_params=params.models,\n",
    "                                    pre_params=params.pre, feature_params=params.features,\n",
    "                                    pipeline_params=params.pipeline,\n",
    "                                    exp_name=log_dir)\n",
    "\n",
    "    elif params.pipeline['type'] == 'crossvalidation':\n",
    "        pipeline = CrossvalidationPipeline(task=params.task, data_params=params.data, feature_params=params.features,\n",
    "                                           model_params=params.models, pre_params=params.pre,\n",
    "                                           pipeline_params=params.pipeline, exp_name=log_dir)\n",
    "    elif params.pipeline['type'] == 'Train_Validate':\n",
    "        pipeline = TrainValidatePipeline(data_params=params.data, model_params=params.models, pre_params=params.pre,\n",
    "                                         feature_params=params.features, pipeline_params=params.pipeline,\n",
    "                                         exp_name=log_dir)\n",
    "\n",
    "    elif params.pipeline['type'] == 'LOOCV':\n",
    "        pipeline = LeaveOneOutPipeline(task=params.task, data_params=params.data, feature_params=params.features,\n",
    "                                       model_params=params.models, pre_params=params.pre,\n",
    "                                       pipeline_params=params.pipeline, exp_name=log_dir)\n",
    "    start = timeit.default_timer()\n",
    "    #pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e3bfa",
   "metadata": {},
   "source": [
    "# Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3ee86c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9572/1125181372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdata_params\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdata_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loading data..1..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdata_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for data_params in pipeline.data_params:\n",
    "    data_id = data_params['id']\n",
    "    # logging\n",
    "    logging.info('loading data..1..')\n",
    "    data = Data(**data_params)\n",
    "\n",
    "    x_train, x_validate_, x_test_, y_train, y_validate_, y_test_, info_train, info_validate_, info_test_, cols = data.get_train_validate_test()\n",
    "\n",
    "    X = np.concatenate((x_train, x_validate_), axis=0)\n",
    "    y = np.concatenate((y_train, y_validate_), axis=0)\n",
    "    info = np.concatenate((info_train, info_validate_), axis=0)\n",
    "\n",
    "    # get model\n",
    "    logging.info('fitting model ...')\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    for model_param in pipeline.model_params:\n",
    "        if 'id' in model_param:\n",
    "            model_name = model_param['id']\n",
    "        else:\n",
    "            model_name = model_param['type']\n",
    "\n",
    "        #set_random_seeds(random_seed=20080808)\n",
    "        model_name = model_name + '_' + data_id\n",
    "        m_param = deepcopy(model_param)\n",
    "        m_param['id'] = model_name\n",
    "        logging.info('fitting model ...')\n",
    "# load model\n",
    "# load model and data --------------------\n",
    "#model_dir = join(base_dir, model_name)\n",
    "from model import model_factory\n",
    "model_file = 'XAI_AGE_ALL'\n",
    "params_file = join(\"_logs/XAI-AGE/RUN1/crossvalidation_average_reg_10_tanh/\", model_file + '_params.yml')\n",
    "print(params_file)\n",
    "loader = DataModelLoader(params_file)\n",
    "nn_model = loader.get_model(model_file)\n",
    "feature_names = nn_model.feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49126af",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4413ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test, y_pred_test_scores = predict(nn_model, X, y)\n",
    "predictions = pd.DataFrame({'ID': info,'Label':np.concatenate(y).ravel(),'Prediction':np.concatenate(y_pred_test).ravel()})\n",
    "predictions = predictions.drop_duplicates(subset=['ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf88e1",
   "metadata": {},
   "source": [
    "Here you can save your predictions to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b39d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25885f2",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"_logs/XAI-AGE/RUN1/crossvalidation_average_reg_10_tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20cfc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#current_dir = dirname(dirname(realpath(__file__)))\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "sys.path.insert(0, dirname(current_dir))\n",
    "\n",
    "#from config_path import *\n",
    "\n",
    "\n",
    "samplename = ''\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "saving_dir = join(current_dir, 'extracted')\n",
    "\n",
    "if not exists(saving_dir):\n",
    "    makedirs(saving_dir)\n",
    "\n",
    "\n",
    "base_dir = join(LOG_PATH, '')\n",
    "model_name = ''\n",
    "\n",
    "importance_type = ['deepexplain_deeplift']\n",
    "target = 'o6'\n",
    "use_data = 'Test'  # {'All', 'Train', 'Test'}\n",
    "dropAR = False\n",
    "\n",
    "layers = ['inputs', 'h0', 'h1', 'h2', 'h3', 'h4', 'h5', 'o_linear6']\n",
    "\n",
    "\n",
    "\n",
    "# load model and data --------------------\n",
    "model_dir = join(base_dir, model_name)\n",
    "model_file = 'XAI_AGE_ALL'\n",
    "params_file = join(model_dir, model_file + '_params.yml')\n",
    "print(params_file)\n",
    "loader = DataModelLoader(params_file)\n",
    "nn_model = loader.get_model(model_file)\n",
    "feature_names = nn_model.feature_names\n",
    "X, Y, info = get_data(loader, use_data, dropAR)\n",
    "response = pd.DataFrame(Y, index=info, columns=['response'])\n",
    "print(response.head())\n",
    "filename = join(saving_dir, 'response.csv')\n",
    "response.to_csv(filename)\n",
    "#\n",
    "print('saving gradeint importance')\n",
    "# #gradeint importance --------------------\n",
    "#node_weights_, node_weights_samples_dfs = get_node_importance(nn_model, X, Y, importance_type[0], target)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = nn_model.model\n",
    "\n",
    "\n",
    "\n",
    "x_train = X\n",
    "y_train = Y\n",
    "info_train = info\n",
    "importance_type_new=importance_type[0]\n",
    "\n",
    "#ret = get_coef_importance(model, x_train, y_train, target=target, feature_importance=importance_type_new, detailed=True)\n",
    "method = importance_type_new.split('_')[1]\n",
    "ret = get_deep_explain_scores(model, x_train, y_train, target, method_name=method, detailed=True)\n",
    "print(type(ret))\n",
    "if type(ret) is tuple:\n",
    "    coef, coef_detailed = ret\n",
    "    print('coef_detailed', len(coef_detailed))\n",
    "\n",
    "else:\n",
    "    coef = ret\n",
    "    # empty\n",
    "    coef_detailed = [c.T for c in coef]\n",
    "\n",
    "node_weights_dfs = {}\n",
    "node_weights_samples_dfs = {}\n",
    "\n",
    "for i, k in enumerate(nn_model.feature_names.keys()):\n",
    "    name = nn_model.feature_names[k]\n",
    "    w = coef[k]\n",
    "    w_samples = coef_detailed[k]\n",
    "    features = get_pathway_names(name)\n",
    "    df = pd.DataFrame(abs(w.ravel()), index=name, columns=['coef'])\n",
    "    layer = pd.DataFrame(index=name)\n",
    "    layer['layer'] = i\n",
    "    # node_weights_dfs.append(df)\n",
    "    node_weights_dfs[k] = df\n",
    "    # layers.append(layer)\n",
    "    df_samples = pd.DataFrame(w_samples, columns=features)\n",
    "    # node_weights_samples_dfs.append(df_samples)\n",
    "    node_weights_samples_dfs[k] = (df_samples)\n",
    "\n",
    "\n",
    "node_weights_=node_weights_dfs\n",
    "node_weights_samples_dfs = node_weights_samples_dfs\n",
    "\n",
    "save_gradient_importance(node_weights_, node_weights_samples_dfs, info_train, str(samplename))\n",
    "#\n",
    "print('saving link weights')\n",
    "# # link weights --------------------\n",
    "link_weights_df = get_link_weights_df_(nn_model.model, feature_names, layers)\n",
    "save_link_weights(link_weights_df, layers[1:])\n",
    "#\n",
    "print('saving activation')\n",
    "# # activation --------------------\n",
    "#layer_outs_dict = nn_model.get_layer_outputs(X)\n",
    "#save_activation(layer_outs_dict, feature_names, info_train)\n",
    "#\n",
    "print('saving graph stats')\n",
    "# # graph stats --------------------\n",
    "stats = get_degrees(link_weights_df, layers[1:])\n",
    "import numpy as np\n",
    "keys = np.sort(stats.keys())\n",
    "for k in keys:\n",
    "    filename = join(saving_dir, 'graph_stats_{}.csv'.format(k))\n",
    "    stats[k].to_csv(filename)\n",
    "# save_graph_stats(degrees,fan_outs, fan_ins)\n",
    "#\n",
    "print('adjust weights with graph stats')\n",
    "\n",
    "# # graph stats --------------------\n",
    "degrees = []\n",
    "for k in keys:\n",
    "    degrees.append(stats[k].degree.to_frame(name='coef_graph'))\n",
    "\n",
    "node_importance = adjust_coef_with_graph_degree(node_weights_, stats, layers[1:-1], saving_dir)\n",
    "\n",
    "with open('extracted_data.pkl', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump([keys, stats, node_weights_,layers,node_importance], f)\n",
    "\n",
    "filename = join(saving_dir, 'node_importance_graph_adjusted_test.csv')\n",
    "node_importance.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e91fc",
   "metadata": {},
   "source": [
    "Your predictions will be saved to the **extracted** folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
